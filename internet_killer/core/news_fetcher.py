import requests
import json
import os
import sys
from dotenv import load_dotenv
from datetime import datetime, date
from typing import List, Dict, Optional
import pandas as pd
import feedparser

try:
    from config.settings import settings
except ImportError:
    load_dotenv()
    class DefaultSettings:
        NEWS_API_KEY = os.getenv("NEWS_API_KEY")
        MAX_NEWS_COUNT = 100
    settings = DefaultSettings()

class GlobalNewsFetcher:
    """NewsAPIë¥¼ í†µí•´ ì „ì„¸ê³„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥"""
    
    def __init__(self):
        self.api_key = settings.NEWS_API_KEY
        self.base_url = "https://newsapi.org/v2"
        self.data_dir = "internet_killer/data"
        self.today_str = date.today().strftime("%Y%m%d")
        self.csv_file = f"{self.data_dir}/news_data_{self.today_str}.csv"
        self.json_file = f"{self.data_dir}/news_data_{self.today_str}.json"
        
        self.rss_sources = [
            {"url": "https://feeds.yna.co.kr/news", "name": "ì—°í•©ë‰´ìŠ¤", "country": "kr"},
            {"url": "https://www.mk.co.kr/rss/30000001/", "name": "ë§¤ì¼ê²½ì œ", "country": "kr"},
            {"url": "http://news.chosun.com/site/data/rss/rss.xml", "name": "ì¡°ì„ ì¼ë³´", "country": "kr"},
            {"url": "http://feeds.bbci.co.uk/news/rss.xml", "name": "BBC", "country": "gb"},
            {"url": "https://www.theguardian.com/world/rss", "name": "Guardian", "country": "gb"},
            {"url": "http://rss.cnn.com/rss/edition.rss", "name": "CNN", "country": "us"},
        ]

        # ë°ì´í„° í´ë” ìƒì„±
        os.makedirs(self.data_dir, exist_ok=True)
        
    def check_today_data_exists(self) -> bool:
        """ì˜¤ëŠ˜ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸"""
        return os.path.exists(self.csv_file) and os.path.exists(self.json_file)
    
    def collect_all_news(self, force_update: bool = False) -> Dict:
        """ì „ ì„¸ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥"""
        
        if self.check_today_data_exists() and not force_update:
            print(f"âœ… ì˜¤ëŠ˜ ë‚ ì§œ({self.today_str})ì˜ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì´ë¯¸ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {self.csv_file}, {self.json_file}")
            return {"status": "already_exists", "date": self.today_str}
            
        if not self.api_key:
            print("âŒ NEWS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            return {"status": "error", "message": "API key missing"}
        
        print(f"ğŸŒ ì „ì„¸ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘... (ë‚ ì§œ: {self.today_str})")
        all_articles = []
        
        # ë‹¤ì–‘í•œ êµ­ê°€ì™€ ì¹´í…Œê³ ë¦¬ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        countries_and_categories = [
            # í•œêµ­ ë‰´ìŠ¤
            {"country": "kr", "category": None, "name": "í•œêµ­ ì „ì²´"},
            {"country": "kr", "category": "technology", "name": "í•œêµ­ ê¸°ìˆ "},
            {"country": "kr", "category": "business", "name": "í•œêµ­ ê²½ì œ"},
            {"country": "kr", "category": "entertainment", "name": "í•œêµ­ ì—°ì˜ˆ"},
            
            # ë¯¸êµ­ ë‰´ìŠ¤  
            {"country": "us", "category": None, "name": "ë¯¸êµ­ ì „ì²´"},
            {"country": "us", "category": "technology", "name": "ë¯¸êµ­ ê¸°ìˆ "},
            {"country": "us", "category": "business", "name": "ë¯¸êµ­ ê²½ì œ"},
            {"country": "us", "category": "politics", "name": "ë¯¸êµ­ ì •ì¹˜"},
            
            # ì˜êµ­ ë‰´ìŠ¤
            {"country": "gb", "category": None, "name": "ì˜êµ­ ì „ì²´"},
            {"country": "gb", "category": "business", "name": "ì˜êµ­ ê²½ì œ"},
            {"country": "gb", "category": "politics", "name": "ì˜êµ­ ì •ì¹˜"},
            
            # ê¸€ë¡œë²Œ ì£¼ìš” ì†ŒìŠ¤ë“¤
            {"sources": "bbc-news,cnn,reuters,associated-press,techcrunch", "name": "ê¸€ë¡œë²Œ ì£¼ìš” ë§¤ì²´"}
        ]
        
        for config in countries_and_categories:
            print(f"ğŸ“° ìˆ˜ì§‘ ì¤‘: {config['name']}")
            articles = self._fetch_news_by_config(config)
            all_articles.extend(articles)
            
        print("ğŸ“¡ RSS í”¼ë“œì—ì„œ ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘...")
        for rss_config in self.rss_sources:
            print(f"ğŸ“° RSS ìˆ˜ì§‘ ì¤‘: {rss_config['name']}")
            rss_articles = self._fetch_rss_news(rss_config)
            all_articles.extend(rss_articles)

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = self._remove_duplicates(all_articles)
        print(f"âœ¨ ì´ {len(unique_articles)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
        
        # ì €ì¥
        self._save_to_files(unique_articles)
        
        return {
            "status": "success", 
            "date": self.today_str,
            "total_articles": len(unique_articles),
            "files": [self.csv_file, self.json_file]
        }
    
    def _fetch_news_by_config(self, config: Dict) -> List[Dict]:
        """ì„¤ì •ì— ë”°ë¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = f"{self.base_url}/top-headlines"
            params = {
                "apiKey": self.api_key,
                "pageSize": 20,  # ê° ì¹´í…Œê³ ë¦¬ë‹¹ 20ê°œì”©
            }

            if config.get("country") == "kr":
                params["country"] = "kr"
            elif config.get("country") == "gb":
                params["country"] = "gb"
                params["language"] = "en"
            elif "sources" in config:
                params["sources"] = config["sources"]
                params["language"] = "en"
            else:
                params["country"] = config["country"]
                params["language"] = "en"

            if config.get("category"):
                params["category"] = config["category"]

            print(f"ğŸ” API ìš”ì²­: {params}")

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            print(f"ğŸ“Š ì‘ë‹µ: {data.get('totalResults', 0)}ê°œ ê²°ê³¼")

            articles = []
            for article in data.get("articles", []):
                if article["title"] and article["description"]:  # ì œëª©ê³¼ ë‚´ìš©ì´ ìˆëŠ” ê²ƒë§Œ
                    processed_article = {
                        "id": f"{config['name']}_{len(articles)}",
                        "title": article["title"],
                        "description": article["description"],
                        "content": article.get("content", "")[:1000] + "..." if article.get("content") else "",
                        "url": article["url"],
                        "source_name": article["source"]["name"],
                        "category": config["name"],
                        "country": config.get("country", "global"),
                        "published_at": article["publishedAt"],
                        "collected_at": datetime.now().isoformat(),
                        "author": article.get("author", "Unknown")
                    }
                    articles.append(processed_article)
                    
            return articles
            
        except Exception as e:
            print(f"âŒ {config['name']} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _fetch_rss_news(self, rss_config: Dict) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"ğŸ” RSS ìš”ì²­: {rss_config['url']}")
        
            feed = feedparser.parse(rss_config['url'])
            articles = []
        
            for entry in feed.entries[:10]:  # RSSì—ì„œëŠ” 10ê°œë§Œ
                processed_article = {
                    "id": f"{rss_config['name']}_{len(articles)}",
                    "title": entry.title,
                    "description": getattr(entry, 'summary', entry.title)[:200] + "...",
                    "content": getattr(entry, 'content', [{}])[0].get('value', '')[:1000] + "..." if hasattr(entry, 'content') else "",
                    "url": entry.link,
                    "source_name": rss_config['name'],
                    "category": f"{rss_config['name']} RSS",
                    "country": rss_config['country'],
                    "published_at": getattr(entry, 'published', datetime.now().isoformat()),
                    "collected_at": datetime.now().isoformat(),
                    "author": getattr(entry, 'author', "Unknown")
                }
                articles.append(processed_article)
            
            print(f"ğŸ“Š RSS ì‘ë‹µ: {len(articles)}ê°œ ê²°ê³¼")
            return articles
        
        except Exception as e:
            print(f"âŒ RSS {rss_config['name']} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _remove_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique_articles = []
        
        for article in articles:
            if article["url"] not in seen_urls:
                seen_urls.add(article["url"])
                unique_articles.append(article)
                
        return unique_articles
    
    def _save_to_files(self, articles: List[Dict]):
        """CSVì™€ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # DataFrame ìƒì„±
            df = pd.DataFrame(articles)
            
            # CSV ì €ì¥
            df.to_csv(self.csv_file, index=False, encoding='utf-8-sig')
            print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {self.csv_file}")
            
            # JSON ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
            json_data = {
                "collection_date": self.today_str,
                "collection_timestamp": datetime.now().isoformat(),
                "total_articles": len(articles),
                "articles": articles
            }
            
            with open(self.json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {self.json_file}")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def load_today_data(self) -> Optional[pd.DataFrame]:
        """ì˜¤ëŠ˜ ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            if os.path.exists(self.csv_file):
                df = pd.read_csv(self.csv_file)
                print(f"ğŸ“Š ë¶ˆëŸ¬ì˜¨ ë°ì´í„°: {len(df)}ê°œ ë‰´ìŠ¤")
                return df
            else:
                print("âŒ ì˜¤ëŠ˜ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None

# ì‹¤í–‰ë¶€
if __name__ == "__main__":
    fetcher = GlobalNewsFetcher()
    
    print("ğŸš€ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    result = fetcher.collect_all_news()
    
    if result["status"] == "success":
        print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {result['total_articles']}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        # ì €ì¥ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        df = fetcher.load_today_data()
        if df is not None:
            print("\nğŸ“‹ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
            category_stats = df['category'].value_counts()
            for category, count in category_stats.items():
                print(f"  â€¢ {category}: {count}ê°œ")

